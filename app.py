import io
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import json

import pandas as pd
import requests
from bs4 import BeautifulSoup
from PIL import Image as PILImage
from io import BytesIO

import streamlit as st
from openpyxl import Workbook
from openpyxl.drawing.image import Image as XLImage

st.set_page_config(page_title="ç™ºæ³¨æ›¸ è‡ªå‹•ç”Ÿæˆï¼ˆæ¥½å¤©ç”»åƒå°‚ç”¨ï¼‰", page_icon="ğŸ§¾", layout="wide")
st.title("ğŸ§¾ ç™ºæ³¨æ›¸ è‡ªå‹•ç”Ÿæˆï½œç”»åƒã¯æ¥½å¤©ã®ã¿ã‹ã‚‰å–å¾—")

# ------- UI -------
col1, col2 = st.columns(2)
up_orders = col1.file_uploader("å—æ³¨CSVï¼ˆåˆ—: sku, è³¼å…¥æ•°ï¼‰", type=["csv"])
up_master = col2.file_uploader("DBï¼ˆExcel: sku, åŸä¾¡, å•†å“URL, å•†å“åç§°, ç‰¹è¨˜äº‹é …ï¼‰", type=["xlsx","xls"])

order_date = st.date_input("ç™ºæ³¨æ—¥", value=datetime.now().date())
img_max_h = st.number_input("ç”»åƒã®æœ€å¤§é«˜ã•(px)", min_value=60, max_value=240, value=120)
img_col_width = st.number_input("Båˆ—ã®å¹…(æ–‡å­—æ•°)", min_value=10, max_value=40, value=18)

st.markdown("---")
left, right = st.columns([2,1])
rakuten_shop = left.text_input("æ¥½å¤©ã®ã‚·ãƒ§ãƒƒãƒ—IDï¼ˆä¾‹: lilirenaï¼‰", value="lilirena")
max_workers = right.slider("ä¸¦åˆ—å–å¾—æ•°", 2, 12, 6)

# ------- HTTP client -------
DEFAULT_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "ja,en;q=0.9",
}

session = requests.Session()
session.headers.update(DEFAULT_HEADERS)

# ------- helpers -------

def normalize_sku(s):
    if pd.isna(s):
        return None
    return str(s).strip().replace("ã€€"," ").lower()

def base_code_from_sku(sku: str) -> str:
    if not sku:
        return ""
    code = str(sku).split("-")[0].strip()
    return code

def build_rakuten_url(sku: str, shop: str) -> str:
    code = base_code_from_sku(sku)
    if not code or not shop:
        return ""
    return f"https://item.rakuten.co.jp/{shop}/{code}/"

def pick_rakuten_image(html: str) -> str | None:
    if not html:
        return None
    soup = BeautifulSoup(html, "html.parser")

    for attrs in (
        {"property": "og:image"},
        {"name": "og:image"},
        {"property": "twitter:image"},
        {"name": "twitter:image"},
        {"property": "og:image:url"},
    ):
        tag = soup.find("meta", attrs=attrs)
        if tag and tag.get("content"):
            u = tag["content"].strip()
            if u.startswith("//"): u = "https:" + u
            return u

    for s in soup.find_all("script", attrs={"type": "application/ld+json"}):
        try:
            data = json.loads(s.string or "{}")
        except Exception:
            continue
        def extract_img(obj):
            if isinstance(obj, dict):
                img = obj.get("image")
                if isinstance(img, str):
                    return img
                if isinstance(img, list) and img:
                    return img[0]
            return None
        if isinstance(data, dict) and data.get("@type") == "Product":
            u = extract_img(data)
            if u:
                return u
        if isinstance(data, list):
            for node in data:
                if isinstance(node, dict) and node.get("@type") == "Product":
                    u = extract_img(node)
                    if u:
                        return u

    for sel in [
        "#rakutenLimitedId_ImageMain img",
        "#productMainImage img",
        "#page-body img",
        "img",
    ]:
        el = soup.select_one(sel)
        if el:
            src = el.get("src") or el.get("data-src") or ""
            if src:
                if src.startswith("//"): src = "https:" + src
                return src

    return None

def download_image(image_url: str, referer: str | None = None) -> BytesIO | None:
    if not image_url:
        return None
    headers = DEFAULT_HEADERS.copy()
    if referer:
        headers["Referer"] = referer
    try:
        resp = session.get(image_url, headers=headers, stream=True, timeout=5)
        resp.raise_for_status()
        ctype = (resp.headers.get("Content-Type") or "").lower()
        if not (ctype.startswith("image/") or any(x in ctype for x in ["webp","jpeg","png","jpg"])):
            return None
        data = BytesIO(resp.content)
        data.seek(0)
        return data
    except Exception:
        return None

def resize_keep_ratio(bin_io: BytesIO, max_h: int) -> BytesIO | None:
    try:
        img = PILImage.open(bin_io)
        if img.mode not in ("RGB","RGBA"):
            img = img.convert("RGB")
        w, h = img.size
        if h > max_h:
            scale = max_h / h
            img = img.resize((int(w*scale), int(h*scale)))
        out = BytesIO()
        img.save(out, format="PNG", optimize=True)
        out.seek(0)
        return out
    except Exception:
        return None

def read_orders(file) -> pd.DataFrame:
    for enc in ["cp932","utf-8-sig","utf-8"]:
        try:
            file.seek(0)
            df = pd.read_csv(file, encoding=enc)
            if {"sku","è³¼å…¥æ•°"}.issubset(df.columns):
                df["sku"] = df["sku"].apply(normalize_sku)
                df["è³¼å…¥æ•°"] = pd.to_numeric(df["è³¼å…¥æ•°"], errors="coerce").fillna(0).astype(int)
                return df
        except Exception:
            pass
    raise ValueError("å—æ³¨CSVã®åˆ—åã¯ 'sku, è³¼å…¥æ•°' ã‚’æƒ³å®šã—ã¦ã„ã¾ã™ã€‚")

if up_orders and up_master:
    try:
        orders = read_orders(up_orders)
        master = pd.read_excel(up_master)
        need = {"sku","åŸä¾¡","å•†å“URL","å•†å“åç§°","ç‰¹è¨˜äº‹é …"}
        if not need.issubset(master.columns):
            st.error(f"DBã®åˆ—åä¸è¶³: å¿…è¦ {need} / å®Ÿéš› {set(master.columns)}")
            st.stop()

        master["sku"] = master["sku"].apply(normalize_sku)
        orders_sum = orders.groupby("sku",as_index=False)["è³¼å…¥æ•°"].sum().query("è³¼å…¥æ•°>0")
        merged = orders_sum.merge(master, on="sku", how="left")

        st.info("ç”»åƒURLã‚’æ¥½å¤©ã‹ã‚‰å–å¾—ä¸­â€¦ï¼ˆè¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ã€ç”»åƒãªã—ã€ï¼‰")

        decided_urls = [None] * len(merged)
        to_fetch = []

        for i, r in merged.iterrows():
            rak_url = build_rakuten_url(r.get("sku"), rakuten_shop)
            if rak_url:
                to_fetch.append((i, rak_url, r.get("sku")))

        def fetch_one(idx: int, url: str, sku: str):
            try:
                resp = session.get(url, timeout=5)
                resp.raise_for_status()
                html = resp.text
                return idx, (pick_rakuten_image(html) or None), url
            except Exception:
                return idx, None, url

        with ThreadPoolExecutor(max_workers=max_workers) as ex:
            futures = [ex.submit(fetch_one, i, u, s) for (i,u,s) in to_fetch]
            for fut in as_completed(futures):
                i, img_u, ref = fut.result()
                decided_urls[i] = img_u

        merged["ç”»åƒURL(æ±ºå®š)"] = [u if u else "ç”»åƒãªã—" for u in decided_urls]

        st.subheader("ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
        st.dataframe(merged[["sku","è³¼å…¥æ•°","å•†å“åç§°","å•†å“URL","ç”»åƒURL(æ±ºå®š)"]], use_container_width=True, height=320)

        st.info("Excelã‚’ç”Ÿæˆä¸­â€¦ï¼ˆç”»åƒã‚’åŸ‹ã‚è¾¼ã¿ï¼‰")
        wb = Workbook()
        ws = wb.active
        ws.title = "ç™ºæ³¨æ›¸"

        headers = ["", "å†™çœŸ", "sku", "è³¼å…¥æ•°", "å˜ä¾¡",
                   "ç‰¹è¨˜äº‹é …", "å•†å“åç§°", "å•†å“URL", "å¤‰æ›´å¾ŒURL",
                   "ã‚µã‚¤ã‚º", "è‰²", "ä¸­å›½å†…é€æ–™",
                   "å˜ä¾¡", "åˆè¨ˆ", "ç™ºæ³¨æ—¥",
                   ""]
        ws.append(headers)
        ws.column_dimensions["B"].width = float(img_col_width)

        failed_rows = []
        embed_ok = 0
        fallback_image_formula = 0
        embed_fail = 0

        d = order_date
        date_str = f"{d.year}/{d.month}/{d.day}"

        for _, row in merged.iterrows():
            genka = pd.to_numeric(row.get("åŸä¾¡"), errors="coerce")
            qty = int(row.get("è³¼å…¥æ•°") or 0)
            gokei = (genka * qty) if pd.notna(genka) else None

            excel_row = ["", "", row.get("sku"), qty, "",
                         row.get("ç‰¹è¨˜äº‹é …"), row.get("å•†å“åç§°"),
                         row.get("å•†å“URL"), "", "", "", "",
                         genka, gokei, date_str, ""]
            ws.append(excel_row)

            r_i = ws.max_row
            img_url = row.get("ç”»åƒURL(æ±ºå®š)")
            referer = build_rakuten_url(row.get("sku"), rakuten_shop)

            bin_data = download_image(img_url, referer=referer) if (img_url and img_url != "ç”»åƒãªã—") else None
            if bin_data:
                bin_resized = resize_keep_ratio(bin_data, max_h=int(img_max_h))
                if bin_resized:
                    try:
                        xlimg = XLImage(bin_resized)
                        ws.add_image(xlimg, f"B{r_i}")
                        ws.row_dimensions[r_i].height = int(int(img_max_h) / 1.33)
                        embed_ok += 1
                        continue
                    except Exception:
                        pass

            if img_url and img_url != "ç”»åƒãªã—":
                try:
                    ws.cell(row=r_i, column=2).value = f'=IMAGE("{img_url}")'
                    ws.row_dimensions[r_i].height = int(int(img_max_h) / 1.33)
                    fallback_image_formula += 1
                except Exception:
                    embed_fail += 1
                    failed_rows.append({"sku": row.get("sku"), "å•†å“URL": row.get("å•†å“URL"), "ç”»åƒURL": img_url, "ç†ç”±": "IMAGEé–¢æ•°ä¿é™ºã‚‚å¤±æ•—"})
            else:
                embed_fail += 1
                failed_rows.append({"sku": row.get("sku"), "å•†å“URL": row.get("å•†å“URL"), "ç”»åƒURL": img_url, "ç†ç”±": "URLãªã—"})

        if failed_rows:
            ws2 = wb.create_sheet("ç”»åƒå–å¾—å¤±æ•—")
            ws2.append(["sku","å•†å“URL","ç”»åƒURL","ç†ç”±"])
            for fr in failed_rows:
                ws2.append([fr.get("sku"), fr.get("å•†å“URL"), fr.get("ç”»åƒURL"), fr.get("ç†ç”±")])

        bio = BytesIO()
        wb.save(bio)
        bio.seek(0)
        filename = f"ç™ºæ³¨æ›¸_{datetime.now().strftime('%Y%m%d')}_ç”»åƒåŸ‹ã‚è¾¼ã¿.xlsx"
        st.download_button("ğŸ“¥ ç™ºæ³¨æ›¸ï¼ˆç”»åƒåŸ‹ã‚è¾¼ã¿ï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                           data=bio.getvalue(),
                           file_name=filename,
                           mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

        st.success(f"ç”»åƒåŸ‹ã‚è¾¼ã¿ æˆåŠŸ: {embed_ok} ä»¶ / ä¿é™º(IMAGEé–¢æ•°): {fallback_image_formula} ä»¶ / å¤±æ•—: {embed_fail} ä»¶")

    except Exception as e:
        st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
else:
    st.info("å—æ³¨CSVã¨DBã‚’é¸ã¶ã¨å‡¦ç†ã§ãã¾ã™ã€‚")